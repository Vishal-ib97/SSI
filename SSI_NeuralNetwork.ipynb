{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSI_NeuralNetwork",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLIxMqs5nLgC",
        "outputId": "45fddec1-3625-424d-9eaf-5d94ac5fd62b"
      },
      "source": [
        "!pip install import_ipynb\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import import_ipynb\n",
        "import preprocess_with_val\n",
        "import numpy as np\n",
        "np.random.seed(530)\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_curve,auc,confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.stats import pearsonr, spearmanr, ks_2samp\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "import random"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvf65XssneXc"
      },
      "source": [
        "race_list = ['patient_race_american_indian_or_alaska_native',\n",
        " 'patient_race_asian',\n",
        " 'patient_race_hispanic',\n",
        " 'patient_race_native_hawaiian_or_other_pacific_islander',\n",
        " 'patient_race_other',\n",
        " 'patient_race_white',\n",
        " 'patient_race_african_american_or_black']\n",
        "\n",
        "# import pickle\n",
        "# f = open(r'columns_60', 'rb')\n",
        "# fin_cols = pickle.load(f)\n",
        "\n",
        "#load dataset\n",
        "cov_train = pd.read_csv(\"preprocessed_covariate_race_matrix_post_surgery_7_days_train_{}.csv\".format(\"1225\"))\n",
        "cov_test = pd.read_csv(\"preprocessed_covariate_race_matrix_post_surgery_7_days_test_{}.csv\".format(\"1225\"))\n",
        "cov_val = pd.read_csv(\"preprocessed_covariate_race_matrix_post_surgery_7_days_val_{}.csv\".format(\"1225\"))\n",
        "\n",
        "cov_train_final, cov_test_final, cov_val_final = preprocess_with_val.preprocess(cov_train, cov_test, cov_val)\n",
        "cov_train_final.reset_index(drop = True, inplace = True)\n",
        "cov_test_final.reset_index(drop = True, inplace =True)\n",
        "cov_val_final.reset_index(drop = True, inplace =True)\n",
        "\n",
        "cov_train_final.drop(race_list + ['patient_gender_male'], axis = 1, inplace = True)\n",
        "cov_test_final.drop(race_list + ['patient_gender_male'], axis = 1, inplace = True)\n",
        "cov_val_final.drop(race_list + ['patient_gender_male'], axis = 1, inplace = True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXzu0xhjnniy"
      },
      "source": [
        "dtype = torch.FloatTensor\n",
        "#model class\n",
        "class logisticregression(torch.nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, seed = 530):\n",
        "    super(logisticregression, self).__init__()\n",
        "    # self.layer = nn.Sequential(nn.Linear(input_dim, output_dim),\n",
        "    #                            nn.Dropout(p = 0.8),\n",
        "    #                            nn.BatchNorm1d(128),\n",
        "    #                            nn.ReLU(),\n",
        "\n",
        "\n",
        "                               \n",
        "    self.seed = seed\n",
        "\n",
        "    self.linear = torch.nn.Linear(input_dim, 64)\n",
        "    self.linear2 = torch.nn.Linear(64, 1)\n",
        "    # self.linear3 = torch.nn.Linear(128, 64)\n",
        "    # self.linear4 = torch.nn.Linear(64, 32)\n",
        "    # self.linear5 = torch.nn.Linear(32, output_dim)\n",
        "    self.bn = nn.BatchNorm1d(input_dim)\n",
        "    self.bn1 = nn.BatchNorm1d(64)\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.drop1 = torch.nn.Dropout(p = 0.8)\n",
        "\n",
        "    \n",
        "    self.linear.apply(self.init_weights)\n",
        "    self.linear2.apply(self.init_weights)\n",
        "    # self.linear3.apply(self.init_weights)\n",
        "    # self.linear4.apply(self.init_weights)\n",
        "    # self.linear5.apply(self.init_weights)\n",
        "\n",
        "\n",
        "  def init_weights(self, m):\n",
        "    if type(m) == nn.Linear:\n",
        "      #m.weight.data.fill_(0.05)\n",
        "      torch.manual_seed(self.seed)\n",
        "      # nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "      # nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out', nonlinearity='relu')\n",
        "      torch.nn.init.orthogonal_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "      # torch.nn.init.sparse_(m.weight, sparsity = 0.1, std = 0.01)\n",
        "      m.bias.data.fill_(0.0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # output1 = self.linear(x)\n",
        "    # x = self.bn(x)\n",
        "    output1 = self.relu(self.bn1(self.drop1(self.linear(x))))\n",
        "    # output1 = self.relu(self.drop1(self.linear(x)))\n",
        "    # output1 = self.bn(output1)\n",
        "    # drop1 = torch.nn.Dropout(p = 0.5)(output1)\n",
        "    # output1 = self.relu(drop1)\n",
        "    \n",
        "\n",
        "    output2 = self.linear2(output1)\n",
        "    # output2 = self.relu(self.bn2(self.drop2(self.linear(output1))))\n",
        "    # output2 = self.relu(output2)\n",
        "    # drop2 = torch.nn.Dropout(p = 0.5)(output2)\n",
        "\n",
        "    # output3 = self.linear3(output2)\n",
        "    # output3 = self.relu(output3)\n",
        "    # drop3 = torch.nn.Dropout(p = 0.3)(output3)\n",
        "\n",
        "    # output4 = self.linear4(drop3)\n",
        "    # output4 = self.relu(output4)\n",
        "    # drop4 = torch.nn.Dropout(p = 0.3)(output4)\n",
        "\n",
        "    # output5 = self.linear5(drop4)\n",
        "    output5 = self.sigmoid(output2)\n",
        "    return output5\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktWXVXCboAlf"
      },
      "source": [
        "loss_func = torch.nn.BCELoss()\n",
        "# cov_train_final, _ = preprocess.preprocess(cov_train, cov_test)\n",
        "train_data = cov_train_final\n",
        "def scale(x_scale, train_scale = cov_train_final):\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  # train_scale = train_data.sample(frac = 1, random_state = 530).reset_index(drop = True)\n",
        "  # train_scale = train_data\n",
        "  scaler = MinMaxScaler()\n",
        "  # scaler = StandardScaler()\n",
        "  X = train_scale.drop(['ssi'], axis = 1)\n",
        "  X = np.array(X)\n",
        "  scaler.fit(X)\n",
        "  res = scaler.transform(x_scale)\n",
        "  res = pd.DataFrame(res, columns = train_scale.drop(['ssi'], axis = 1).columns)\n",
        "  return res"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlBllzi7oEC1"
      },
      "source": [
        "def train_dataloader(batch = 1000, train_data = cov_train_final):\n",
        "  # print(batch)\n",
        "  train = train_data.sample(frac = 1, random_state = 530).reset_index(drop = True)\n",
        "  X_train = train.drop(['ssi'], axis = 1)\n",
        "  X_train = scale(X_train.values)\n",
        "  X_train = X_train.values\n",
        "  y_train = train['ssi'].values\n",
        "\n",
        "  # batch = 1000\n",
        "  if batch == \"all\":\n",
        "      yield X_train, y_train\n",
        "  else:\n",
        "    idx = 0\n",
        "    while idx < len(X_train):\n",
        "      if idx + batch < len(X_train):\n",
        "        yield X_train[idx:idx + batch, :], y_train[idx:idx + batch]\n",
        "      else:\n",
        "        yield X_train[idx:len(X_train), :], y_train[idx:len(X_train)]\n",
        "      idx = idx + batch"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rkYpTtVoIOA"
      },
      "source": [
        "def conf_mat(test, pred):\n",
        "  conf_matrix = confusion_matrix(test, pred)\n",
        "  TP = conf_matrix[1][1]\n",
        "  TN = conf_matrix[0][0]\n",
        "  FP = conf_matrix[0][1]\n",
        "  FN = conf_matrix[1][0]\n",
        "  return TP, TN, FP, FN"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5xsfTdHoLdL"
      },
      "source": [
        "def weighted_cross_entropy(y_pred, y_true):\n",
        "  logloss = (1-y_true)*torch.log(1e-6 + 1 - y_pred)\n",
        "  logloss += y_true*torch.log(1e-6 + y_pred)\n",
        "  return -torch.mean(logloss)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5Vj9AjYoSsX"
      },
      "source": [
        "def predict(p_model, x, logit = False, thresh = 0.5):\n",
        "  p_model.eval()\n",
        "  x_ = x.drop(['ssi'], axis = 1).values\n",
        "  y_ = x['ssi'].values\n",
        "  x_= scale(x_)\n",
        "  x_ = x_.values\n",
        "  x_ = Variable(torch.from_numpy(x_).type(dtype))\n",
        "  y_ = Variable(torch.from_numpy(y_).type(dtype))\n",
        "  y_ = y_.view(x_.size()[0], )\n",
        "  output_g = p_model(x_)\n",
        "  y_pred = output_g\n",
        "  # y_pred = output_g.detach().numpy()\n",
        "  if logit:\n",
        "    y_pred[y_pred >= thresh] = 1\n",
        "    y_pred[y_pred < thresh] = 0\n",
        "  return y_pred, y_"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-vRhX0UoZgl"
      },
      "source": [
        "#baseline performance\n",
        "from torch import autograd\n",
        "output_dim = 1\n",
        "epochs = 100\n",
        "lr_rate = 1e-2\n",
        "\n",
        "#input_dim = len(fin_cols) - 1\n",
        "input_dim = 264\n",
        "mod = logisticregression(input_dim = input_dim, output_dim = output_dim)\n",
        "\n",
        "#for i, r in enumerate(race_list):\n",
        "optimizer = torch.optim.Adam(mod.parameters(), lr = 1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr = 1e-4, max_lr = 5e-4, cycle_momentum = False)\n",
        "# lambda1 = lambda epoch: 0.65 ** epoch\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 10, eta_min=1e-4)   #issa good\n",
        "# scheduler =  torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-4, max_lr=1e-2,step_size_up=5,mode=\"exp_range\", cycle_momentum = False)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=35, epochs=100)\n",
        "min_val_loss = float('inf')\n",
        "batch_loss = []\n",
        "#training loop\n",
        "opt_array = []\n",
        "loss_array = []\n",
        "for epoch in range(epochs):\n",
        "    print(\"epoch: \", epoch)\n",
        "    \n",
        "    b = 0\n",
        "    # if (epoch+1)%10 == 0:\n",
        "    #   optimizer.param_groups[0]['lr'] /= 10\n",
        "    # for feat, label in train_dataloader():\n",
        "    for feat, label in train_dataloader(batch = 256):\n",
        "    # for feat, label in batch_data.values():\n",
        "      mod.train()\n",
        "      # feat, label = batch_data[i]\n",
        "      b += 1\n",
        "      x = Variable(torch.from_numpy(feat).type(dtype))\n",
        "      y = Variable(torch.from_numpy(label).type(dtype))\n",
        "      #with autograd.detect_anomaly():\n",
        "      optimizer.zero_grad()\n",
        "      outputs = mod(x)\n",
        "      y = torch.reshape(y, outputs.shape)\n",
        "      loss = loss_func(outputs, y) \n",
        "      loss_array.append(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "    opt_array.append(optimizer.param_groups[0]['lr'])\n",
        "    # print(b)\n",
        "    print(\"loss: \", sum(loss_array).detach().numpy()/len(loss_array))\n",
        "    batch_loss.append(sum(loss_array).detach().numpy()/len(loss_array))\n",
        "    y_pred_val, y_val = predict(mod, cov_val_final)\n",
        "    y_val = torch.reshape(y_val, y_pred_val.shape)\n",
        "    val_loss = loss_func(y_pred_val, y_val)\n",
        "    print(\"val_loss: \", val_loss.detach().numpy())\n",
        "    if val_loss < min_val_loss:\n",
        "      min_val_loss = val_loss\n",
        "      torch.save(mod, 'best_model')\n",
        "      print(\"saving model due to new min val loss\")\n",
        "      best_epoch = epoch\n",
        "    print(\"best_epoch: \", best_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_4VxlZZobrK",
        "outputId": "2445c7bb-21a5-4063-e04b-b48ef97ea617"
      },
      "source": [
        "best_mod = torch.load('best_model')\n",
        "#test data\n",
        "# best_mod.eval()\n",
        "y_pred, y_test = predict(best_mod, cov_test_final)\n",
        "y_pred = y_pred.view(-1)\n",
        "print(loss_func(y_pred, y_test))\n",
        "y_pred = y_pred.detach().numpy()\n",
        "y_test = y_test.detach().numpy()\n",
        "\n",
        "#train data\n",
        "y_pred_train, y_train = predict(best_mod, cov_train_final)\n",
        "y_pred_train = y_pred_train.detach().numpy()\n",
        "y_train = y_train.detach().numpy()\n",
        "\n",
        "pr, re, thresh_test = precision_recall_curve(y_test, y_pred)\n",
        "print(\"AUPRC test: \", auc(re, pr))\n",
        "\n",
        "pr, re, thresh_test = precision_recall_curve(y_train, y_pred_train)\n",
        "print(\"AUPRC train: \", auc(re, pr))\n",
        "\n",
        "fpr_test, tpr_test, _ = roc_curve(y_test, y_pred)\n",
        "print(\"ROC test: \", auc(fpr_test, tpr_test))\n",
        "\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_train)\n",
        "print(\"ROC train: \", auc(fpr_train, tpr_train))\n",
        "\n",
        "y_pred[y_pred >= 0.4] = 1\n",
        "y_pred[y_pred < 0.4] = 0\n",
        "TP, TN, FP, FN = conf_mat(y_test, y_pred)\n",
        "rec = (TP / float(TP + FN))\n",
        "prec = (TP / float(TP + FP))\n",
        "print(\"F1 score: \", 2*rec*prec/(rec+prec))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.2065, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "AUPRC test:  0.20478487094276046\n",
            "AUPRC train:  0.3135030932836749\n",
            "ROC test:  0.8541254636944233\n",
            "ROC train:  0.8922915557325474\n",
            "F1 score:  0.2901960784313726\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}